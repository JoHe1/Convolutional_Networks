{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTUNA: \n",
    "\n",
    "Optuna es una biblioteca de optimización automática de hiperparámetros. El objetivo principal es encontrar la mejor combinación de valores de los hiperparámetros, para aumentar la precisión del modelo. \n",
    "\n",
    "Se definen los hiperparámetros a optimizar, como el tamaño del lote, la tasa de aprendizaje, los filtros de las capas convolutivas, las neuronas de las capas densas y la funcion de activacion, que en este caso son:\n",
    "- ReLU: Devuelve el valor de entrada si es positivo o cero si es negativo\n",
    "- Leaky ReLU: Variante de la ReLU, que en vez de convertir los números negativos a cero, los hace muy pequeños, para que no se \"apaguen\" por completo las neuronas, para que no se desactiven por completo, lo cual hace que deje de contribuir en el aprendizaje.\n",
    "\n",
    "Para el estudio con optuna, se hace una optimización con 20 trials, en las que se van probando distintas combinaciones de hiperparámetros, y al final se imprimen los mejores hiperparámetros encontrados. Se crea una especie de estudio, donde se registrarámn los trials y sus resultados, y al poner direction=\"maximize\", le estamos indicando que el objetivo es aumentar la precisión.  \n",
    "\n",
    "A parte de esto, tenemos la función objetivo, que es la función principal de Optuna, que entrena el modelo de la red con las distintas combinaciones de hiperparámetros, y devuelve la precisión del modelo en el conjunto de prueba. El entrenamiento funciona de la siguiente manera: para cada conjunto de hiperparámetros, el modelo se entrena y calcula la precisión en el conjunto de prueba final. Optuna realiza 20 trials, que son entrenamientos con distintas combinaciones de hiperparámetros, y va ajustando los hiperparámetros. \n",
    "\n",
    "Los resultados se devuelven al final de la función objective, imprimiendo la precisión del conjunto de prueba, y optuna usa este valor para ver cóm de buenos son los hiperparámetros que se están probando.\n",
    "\n",
    "Entre las ventajas de Optuna está la optimización automática de los hiperparámetros, ya que manualemnte es poco eficiente y mucho más trabajo. Además de ello usa estrategias más avanzadas como la búsqueda aleatoria y el pruning, que es una técnica que detiene los ensayos que no muestranb progreso prometedor temprano, lo cual ahorra tiempo y recursos ya que evita entrenamientos innecesarios, y permite encontrar soluciones óptimas más rápido.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Definir la función objetivo para Optuna\n",
    "def objective(trial):\n",
    "    # Hiperparámetros para optimizar\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    lr = trial.suggest_loguniform('lr', 0.001, 0.1)\n",
    "    patience = trial.suggest_int('patience', 3, 10)\n",
    "    num_filters_1 = trial.suggest_int('num_filters_1', 16, 64, step=16)\n",
    "    num_filters_2 = trial.suggest_int('num_filters_2', 32, 128, step=32)\n",
    "    num_filters_3 = trial.suggest_int('num_filters_3', 64, 256, step=64)\n",
    "    num_neurons_1 = trial.suggest_int('num_neurons_1', 64, 256, step=64)\n",
    "    num_neurons_2 = trial.suggest_int('num_neurons_2', 32, 128, step=32)\n",
    "    activation_function = trial.suggest_categorical('activation_function', ['relu', 'leaky_relu'])\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Redimensionar\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Voltear horizontalmente\n",
    "        transforms.RandomRotation(20),  # Rotar aleatoriamente hasta 20 grados\n",
    "        transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Recortar aleatoriamente\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Cambios en color\n",
    "        transforms.ToTensor(),  # Convertir a tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizar\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Redimensionar\n",
    "        transforms.ToTensor(),  # Convertir a tensor\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizar\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(root='dataset_animales/animals/animals', transform=train_transform)\n",
    "    test_dataset = datasets.ImageFolder(root='dataset_animales/animals/animals', transform=test_transform)\n",
    "\n",
    "    train_size = int(0.8 * len(train_dataset))\n",
    "    test_size = len(train_dataset) - train_size\n",
    "\n",
    "    train_data, _ = random_split(train_dataset, [train_size, test_size])\n",
    "    _, test_data = random_split(test_dataset, [train_size, test_size])\n",
    "\n",
    "    # Guardar los datos para entrenar y testear\n",
    "    train_data = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_data = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Modelo con tres capas convolutivas\n",
    "    class ConvNet(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ConvNet, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(3, num_filters_1, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv2 = nn.Conv2d(num_filters_1, num_filters_2, kernel_size=3, stride=1, padding=1)\n",
    "            self.conv3 = nn.Conv2d(num_filters_2, num_filters_3, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc1 = nn.Linear(num_filters_3 * 28 * 28, num_neurons_1)\n",
    "            self.fc2 = nn.Linear(num_neurons_1, num_neurons_2)\n",
    "            self.fc3 = nn.Linear(num_neurons_2, 7)\n",
    "            self.activation_function = activation_function\n",
    "\n",
    "        def forward(self, x):\n",
    "            activation = F.relu if self.activation_function == 'relu' else F.leaky_relu\n",
    "            x = self.pool(activation(self.conv1(x)))\n",
    "            x = self.pool(activation(self.conv2(x)))\n",
    "            x = self.pool(activation(self.conv3(x)))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = self.dropout(activation(self.fc1(x)))\n",
    "            x = activation(self.fc2(x))\n",
    "            x = self.fc3(x) \n",
    "            return x\n",
    "\n",
    "    model = ConvNet()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Entrenamiento\n",
    "    best_test_accuracy = 0\n",
    "\n",
    "    # Listas para guardar loss y accuracy\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    epochs = 10\n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for images, labels in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_data)\n",
    "        train_accuracy = 100 * correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Test\n",
    "        model.eval() \n",
    "        running_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_data:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_test += labels.size(0)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "        test_loss = running_loss / len(test_data)\n",
    "        test_accuracy = 100 * correct_test / total_test\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Mostrar el resultado al final de cada época\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "        print(f'Epoch {epoch+1}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "        # Actualizar mejor precisión de validación\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "\n",
    "    return best_test_accuracy\n",
    "\n",
    "# Crear estudio Optuna\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros\n",
    "print('Best hyperparameters:', study.best_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
